{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+u1ry+w1dFTxogEJSb3NS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegith45/Decentralized-learning/blob/main/HFL_VFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H75AFuvNuJ59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8f9bf-6721-41e6-91b0-2be676d01291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Setup and Utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Define Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # One convolution layer\n",
        "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(10 * 12 * 12, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 10 * 12 * 12)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sTiwyoF6upwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Load Dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "X2ahNNZhup1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d843d448-1c61-4d91-fa5a-8da1e1d8e81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.58MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 130kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.18MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4.1: IID Partitioning\n",
        "def iid_partition(dataset, num_clients):\n",
        "    \"\"\"\n",
        "    Split dataset equally at random among clients (IID).\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset) / num_clients)\n",
        "    all_indices = np.random.permutation(len(dataset))\n",
        "    client_dict = {i: all_indices[i*num_items:(i+1)*num_items] for i in range(num_clients)}\n",
        "    return client_dict\n",
        "\n",
        "# Part 4.2: Non-IID Partitioning\n",
        "\n",
        "def noniid_partition(dataset, num_clients, num_shards=20):\n",
        "    labels = np.array(dataset.targets)\n",
        "    indices = np.arange(len(dataset))\n",
        "    idxs_labels = np.vstack((indices, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]  # sort by labels\n",
        "\n",
        "    shards_per_client = num_shards // num_clients\n",
        "    shard_size = len(dataset) // num_shards\n",
        "\n",
        "    client_dict = {i: np.array([], dtype=int) for i in range(num_clients)}\n",
        "    shard_indices = np.arange(num_shards)\n",
        "    np.random.shuffle(shard_indices)\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        shard_ids = shard_indices[i*shards_per_client:(i+1)*shards_per_client]\n",
        "        for sid in shard_ids:\n",
        "            client_dict[i] = np.concatenate((client_dict[i], idxs_labels[0, sid*shard_size:(sid+1)*shard_size]), axis=0)\n",
        "\n",
        "    return client_dict\n"
      ],
      "metadata": {
        "id": "oWo4a0Xwup6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Part 4.3: HFL Partitioning\n",
        "# ------------------------------\n",
        "def hfl_partition(dataset, num_clients):\n",
        "    \"\"\"\n",
        "    Horizontal FL = partition by rows (samples).\n",
        "    Similar to IID but simulates client-specific datasets.\n",
        "    \"\"\"\n",
        "    return iid_partition(dataset, num_clients)\n",
        "\n",
        "# ------------------------------\n",
        "# Part 4.4: VFL Partitioning\n",
        "# ------------------------------\n",
        "def vfl_partition(dataset, num_clients):\n",
        "    \"\"\"\n",
        "    Vertical FL = partition by features (columns).\n",
        "    Each client gets different parts of the image.\n",
        "    For simplicity, split the 28x28 image into vertical chunks.\n",
        "    \"\"\"\n",
        "    feature_splits = np.array_split(np.arange(28), num_clients)\n",
        "\n",
        "    return feature_splits  # returns feature column ranges for each client\n"
      ],
      "metadata": {
        "id": "f386MwPjup9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: Local Training\n",
        "\n",
        "def local_train(model, dataset, indices, epochs=1, batch_size=32, lr=0.01):\n",
        "    \"\"\"\n",
        "    Train the model locally on a client's dataset.\n",
        "    \"\"\"\n",
        "    model = copy.deepcopy(model)\n",
        "    model.train()\n",
        "    loader = DataLoader(Subset(dataset, indices), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model.state_dict()\n"
      ],
      "metadata": {
        "id": "ItW7PfkTvIZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6: FedAvg Aggregation\n",
        "\n",
        "def fedavg(client_weights, client_sizes):\n",
        "    # Weighted average of client weights (FedAvg).\n",
        "    new_state = copy.deepcopy(client_weights[0])\n",
        "    total_size = sum(client_sizes)\n",
        "\n",
        "    for key in new_state.keys():\n",
        "        new_state[key] = sum([client_weights[i][key] * (client_sizes[i]/total_size)\n",
        "                              for i in range(len(client_weights))])\n",
        "    return new_state\n"
      ],
      "metadata": {
        "id": "xRycQEp1vMeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 7: Evaluation\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, preds = torch.max(output, 1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "RKww3HhYvWdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Part 8: Federated Training Loop\n",
        "# ------------------------------\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def federated_train(dataset, test_dataset, partition_type=\"iid\", num_clients=5, rounds=5, local_epochs=1, batch_size=32, lr=0.01):\n",
        "    \"\"\"\n",
        "    Run federated training under given partitioning scheme.\n",
        "    \"\"\"\n",
        "    # --- Partition data ---\n",
        "    if partition_type == \"iid\":\n",
        "        client_dict = iid_partition(dataset, num_clients)\n",
        "    elif partition_type == \"noniid\":\n",
        "        client_dict = noniid_partition(dataset, num_clients)\n",
        "    elif partition_type == \"hfl\":\n",
        "        client_dict = hfl_partition(dataset, num_clients)\n",
        "    elif partition_type == \"vfl\":\n",
        "        # for simplicity, simulate VFL as IID for now (feature-split needs advanced handling)\n",
        "        client_dict = iid_partition(dataset, num_clients)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown partition type!\")\n",
        "\n",
        "    # --- Init global model ---\n",
        "    global_model = SimpleCNN().to(device)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    acc_list = []\n",
        "\n",
        "    # --- Federated rounds ---\n",
        "    for r in range(rounds):\n",
        "        client_weights, client_sizes = [], []\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            indices = client_dict[client_id]\n",
        "            local_state = local_train(global_model, dataset, indices, epochs=local_epochs, batch_size=batch_size, lr=lr)\n",
        "            client_weights.append(local_state)\n",
        "            client_sizes.append(len(indices))\n",
        "\n",
        "        # Aggregate with FedAvg\n",
        "        global_weights = fedavg(client_weights, client_sizes)\n",
        "        global_model.load_state_dict(global_weights)\n",
        "\n",
        "        # Evaluate\n",
        "        acc = evaluate(global_model, test_loader)\n",
        "        acc_list.append(acc)\n",
        "        print(f\"Round {r+1}/{rounds}, Test Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    return acc_list\n"
      ],
      "metadata": {
        "id": "RO0EJRcAwaVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 9: Run Experiments\n",
        "\n",
        "rounds = 5\n",
        "num_clients = 5\n",
        "local_epochs = 2\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\n--- Training with IID ---\")\n",
        "results[\"IID\"] = federated_train(train_dataset, test_dataset, partition_type=\"iid\", num_clients=num_clients, rounds=rounds, local_epochs=local_epochs)\n",
        "\n",
        "print(\"\\n--- Training with Non-IID ---\")\n",
        "results[\"Non-IID\"] = federated_train(train_dataset, test_dataset, partition_type=\"noniid\", num_clients=num_clients, rounds=rounds, local_epochs=local_epochs)\n",
        "\n",
        "print(\"\\n--- Training with HFL ---\")\n",
        "results[\"HFL\"] = federated_train(train_dataset, test_dataset, partition_type=\"hfl\", num_clients=num_clients, rounds=rounds, local_epochs=local_epochs)\n",
        "\n",
        "print(\"\\n--- Training with VFL ---\")\n",
        "results[\"VFL\"] = federated_train(train_dataset, test_dataset, partition_type=\"vfl\", num_clients=num_clients, rounds=rounds, local_epochs=local_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hlqMs1YwaZL",
        "outputId": "e94fb664-e55e-485c-fb3b-2dcfbdb45fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training with IID ---\n",
            "Round 1/5, Test Accuracy = 89.14%\n",
            "Round 2/5, Test Accuracy = 91.44%\n",
            "Round 3/5, Test Accuracy = 92.56%\n",
            "Round 4/5, Test Accuracy = 93.64%\n",
            "Round 5/5, Test Accuracy = 94.36%\n",
            "\n",
            "--- Training with Non-IID ---\n",
            "Round 1/5, Test Accuracy = 53.87%\n",
            "Round 2/5, Test Accuracy = 64.42%\n",
            "Round 3/5, Test Accuracy = 75.14%\n",
            "Round 4/5, Test Accuracy = 80.42%\n",
            "Round 5/5, Test Accuracy = 85.18%\n",
            "\n",
            "--- Training with HFL ---\n",
            "Round 1/5, Test Accuracy = 89.61%\n",
            "Round 2/5, Test Accuracy = 91.63%\n",
            "Round 3/5, Test Accuracy = 93.26%\n",
            "Round 4/5, Test Accuracy = 93.93%\n",
            "Round 5/5, Test Accuracy = 94.78%\n",
            "\n",
            "--- Training with VFL ---\n",
            "Round 1/5, Test Accuracy = 89.41%\n",
            "Round 2/5, Test Accuracy = 91.63%\n",
            "Round 3/5, Test Accuracy = 92.34%\n",
            "Round 4/5, Test Accuracy = 93.14%\n",
            "Round 5/5, Test Accuracy = 94.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xqwq1sjyVCmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}